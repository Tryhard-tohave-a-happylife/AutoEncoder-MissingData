{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "024dd66c-c0ed-4a1d-8e70-51f2b5c4368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.optimizers as Optimizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import keras_tuner as kt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from AutoEncoder import DAE, VAE, CAE, DuoLossAE\n",
    "from TargetEncoder import KFoldTargetEncoderTrain\n",
    "import HandleData\n",
    "\n",
    "import json\n",
    "\n",
    "import sys\n",
    "import sklearn.neighbors._base\n",
    "sys.modules['sklearn.neighbors.base'] = sklearn.neighbors._base\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from missingpy import MissForest\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use(\"ggplot\")\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d4b38e-c2f2-4988-9732-244ba9fdf991",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tf.config.list_physical_devices(\"GPU\"))\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c94e97-81c0-45d8-8065-56c7579a477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Đọc dữ liệu\n",
    "list_data = []\n",
    "for url_data in os.listdir(\"Data\"):\n",
    "    if \"csv\" in url_data:\n",
    "        dt = pd.read_csv(\"Data/{}\".format(url_data))\n",
    "        dt = dt.rename(columns={each: str(each) for each in dt.columns.to_list()})\n",
    "        list_data.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790dd798-b651-4677-a977-a62bd2c97092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_missing_label_data(df, label, thre = 0.2, mechanism = \"mcar\", method = \"uniform\", norm_method=True):\n",
    "    # df = df.drop(columns=[label])\n",
    "    df[label] = LabelEncoder().fit_transform(df[label])\n",
    "    label_df = df[label].values\n",
    "    cat_cols = [i for i in df.select_dtypes(include='object').columns if i != label]\n",
    "    num_cols = [i for i in df.select_dtypes(include=['int64', 'float64']).columns if df[i].nunique() != 2 and i != label]\n",
    "    binary_cols = [i for i in df.select_dtypes(include=['int64', 'float64']).columns if df[i].nunique() == 2 and i != label]\n",
    "    #Missing data\n",
    "    df, mask = HandleData.missing_method(df, mechanism, method, thre)\n",
    "    data, new_col = label_df.reshape(len(label_df), 1), [label]\n",
    "    #Normalize data\n",
    "    std, col_target = None, None\n",
    "    if norm_method is True:\n",
    "        if len(num_cols) != 0:\n",
    "            std = StandardScaler().fit(df[num_cols])\n",
    "            data_std = std.transform(df[num_cols])\n",
    "            new_col = [*new_col, *num_cols]\n",
    "            data = np.hstack((data, data_std))\n",
    "        if len(cat_cols) != 0:\n",
    "            for each in cat_cols:\n",
    "                targetc = KFoldTargetEncoderTrain(each, label, n_fold=10, verbosity=False)\n",
    "                df = targetc.fit_transform(df)\n",
    "            col_target = [each + \"_tar\" for each in cat_cols]\n",
    "            new_col = [*new_col, *col_target]\n",
    "            data = np.hstack((data, df[col_target]))\n",
    "        if len(binary_cols):\n",
    "            new_col = [*new_col, *binary_cols]\n",
    "            data = np.hstack((data, df[binary_cols]))\n",
    "    \n",
    "    if norm_method is True:\n",
    "        df = pd.DataFrame(\n",
    "            data = data, columns = new_col\n",
    "        )\n",
    "    cat_cols = col_target\n",
    "    return df, mask, cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a68742-4c51-4245-8457-edd8ebebf4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tạo missing data, những giá trị dạng categorical được encode bằng target encoding\n",
    "list_20per_miss = []\n",
    "list_40per_miss = []\n",
    "list_60per_miss = []\n",
    "for idx, each in enumerate(list_data):\n",
    "    #20% missing data\n",
    "    df_new_per20_1, _, cat_cols = create_missing_label_data(each, 'label', thre=0.2, mechanism = \"mcar\", method = \"uniform\")\n",
    "    df_new_per20_2, _, cat_cols = create_missing_label_data(each, 'label', thre=0.2, mechanism = \"mcar\", method = \"random\")\n",
    "    df_new_per20_3, _, cat_cols = create_missing_label_data(each, 'label', thre=0.2, mechanism = \"mnar\", method = \"uniform\")\n",
    "    df_new_per20_4, _, cat_cols = create_missing_label_data(each, 'label', thre=0.2, mechanism = \"mnar\", method = \"random\")\n",
    "    #40% missng data\n",
    "    df_new_per40_1, _, cat_cols = create_missing_label_data(each, 'label', thre=0.4, mechanism = \"mcar\", method = \"uniform\")\n",
    "    df_new_per40_2, _, cat_cols = create_missing_label_data(each, 'label', thre=0.4, mechanism = \"mcar\", method = \"random\")\n",
    "    df_new_per40_3, _, cat_cols = create_missing_label_data(each, 'label', thre=0.4, mechanism = \"mnar\", method = \"uniform\")\n",
    "    df_new_per40_4, _, cat_cols = create_missing_label_data(each, 'label', thre=0.4, mechanism = \"mnar\", method = \"random\")\n",
    "    #60% missing data\n",
    "    df_new_per60_1, _, cat_cols = create_missing_label_data(each, 'label', thre=0.6, mechanism = \"mcar\", method = \"uniform\")\n",
    "    df_new_per60_2, _, cat_cols = create_missing_label_data(each, 'label', thre=0.6, mechanism = \"mcar\", method = \"random\")\n",
    "    df_new_per60_3, _, cat_cols = create_missing_label_data(each, 'label', thre=0.6, mechanism = \"mnar\", method = \"uniform\")\n",
    "    df_new_per60_4, _, cat_cols = create_missing_label_data(each, 'label', thre=0.6, mechanism = \"mnar\", method = \"random\")\n",
    "    \n",
    "    obj_1 = {\"mcar_uni\": df_new_per20_1, \"mcar_ran\": df_new_per20_2, \"mnar_uni\": df_new_per20_3, \"mnar_ran\": df_new_per20_4, \"cat_cols\": cat_cols}\n",
    "    obj_2 = {\"mcar_uni\": df_new_per40_1, \"mcar_ran\": df_new_per40_2, \"mnar_uni\": df_new_per40_3, \"mnar_ran\": df_new_per40_4, \"cat_cols\": cat_cols}\n",
    "    obj_3 = {\"mcar_uni\": df_new_per60_1, \"mcar_ran\": df_new_per60_2, \"mnar_uni\": df_new_per60_3, \"mnar_ran\": df_new_per60_4, \"cat_cols\": cat_cols}\n",
    "    list_20per_miss.append(obj_1)\n",
    "    list_40per_miss.append(obj_2)\n",
    "    list_60per_miss.append(obj_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab37b9f2-50e7-4f7c-80d6-876b94cf5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_for_classification(num_class, learning_rate=0.005):\n",
    "    model = Sequential([\n",
    "        Dense(units=128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(rate=0.2),\n",
    "        Dense(units=64, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "        BatchNormalization(),\n",
    "        Dropout(rate=0.1),\n",
    "        Dense(num_class, activation=\"softmax\")\n",
    "    ])\n",
    "    adam = Optimizer.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=adam, loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7721783f-c7fa-4f51-bf45-a28797c3b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def result_each_method(model, X_, y_):\n",
    "    earlyStop = EarlyStopping(monitor=\"loss\", patience=8, mode=\"min\")\n",
    "    model.fit(X_[0], y_[0], epochs=80, batch_size=128, verbose=-1, callbacks=[earlyStop])\n",
    "    pred = model.predict(X_[1])\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "    return accuracy_score(y_[1], pred), f1_score(y_[1], pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1032c2-6450-4744-9deb-b89a1284b06d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Thử nghiệm với KNN, MissForest, MICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee43e5b-c8e9-4f68-845b-f43265ff5b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_split_norm_method(df, col_cate, label='label'):\n",
    "    label_ = df[label]\n",
    "    df = df.drop(columns=[label])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, label_, test_size=0.2, random_state=209)\n",
    "    #KNNimputation\n",
    "    knn_imputer = KNNImputer(n_neighbors=10).fit(X_train)\n",
    "    X_train_knnimp = knn_imputer.transform(X_train)\n",
    "    X_test_knnimp = knn_imputer.transform(X_test)\n",
    "    #MissForest\n",
    "    X_train_missForest, X_test_missForest = X_train.copy(), X_test.copy()\n",
    "    if col_cate is not None:\n",
    "        X_train_missForest[col_cate] = X_train_missForest[col_cate].astype('category')\n",
    "        X_test_missForest[col_cate] = X_test_missForest[col_cate].astype('category')\n",
    "    rdf_imputer = MissForest(max_depth=5, random_state=209, max_iter=8, verbose=0).fit(X_train_missForest)\n",
    "    X_train_missForest = rdf_imputer.transform(X_train_missForest)\n",
    "    X_test_missForest = rdf_imputer.transform(X_test_missForest)\n",
    "    #MICE\n",
    "    mice_imputer = IterativeImputer(random_state=209, max_iter=8, verbose=-1).fit(X_train)\n",
    "    X_train_mice = mice_imputer.transform(X_train)\n",
    "    X_test_mice = mice_imputer.transform(X_test)\n",
    "    return (X_train_knnimp, X_test_knnimp), (X_train_missForest, X_test_missForest), (X_train_mice, X_test_mice), (y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5927e5f-552f-4044-ab07-4e3209dd7487",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_20_res, list_40_res, list_60_res = [], [], []\n",
    "for idx, each in enumerate(list_20per_miss[:1]):\n",
    "    for key, value in list_20per_miss[idx].items():\n",
    "        if key == \"cat_cols\":\n",
    "            continue\n",
    "        knnimp_per20,  msForest_per20, mice_per20, label_set = fill_missing_split_norm_method(value, list_20per_miss[idx][\"cat_cols\"])\n",
    "        acc_knn, f1_knn = result_each_method(model_for_classification(len(set(label_set[0]))), knnimp_per20, label_set)\n",
    "        acc_rdf, f1_rdf = result_each_method(model_for_classification(len(set(label_set[0]))), msForest_per20, label_set)\n",
    "        acc_mice, f1_mice = result_each_method(model_for_classification(len(set(label_set[0]))), mice_per20, label_set)\n",
    "        obj = {\"Data\": idx, \"type\": key, \"KNN\": (acc_knn, f1_knn), \"RDF\": (acc_rdf, f1_rdf), \"MICE\": (acc_mice, f1_mice)}\n",
    "        list_20_res.append(obj)\n",
    "    #40% missng data\n",
    "    for key, value in list_40per_miss[idx].items():\n",
    "        if key == \"cat_cols\":\n",
    "            continue\n",
    "        knnimp_per40,  msForest_per40, mice_per40, label_set = fill_missing_split_norm_method(value, list_40per_miss[idx][\"cat_cols\"])\n",
    "        acc_knn, f1_knn = result_each_method(model_for_classification(len(set(label_set[0]))), knnimp_per40, label_set)\n",
    "        acc_rdf, f1_rdf = result_each_method(model_for_classification(len(set(label_set[0]))), msForest_per40, label_set)\n",
    "        acc_mice, f1_mice = result_each_method(model_for_classification(len(set(label_set[0]))), mice_per40, label_set)\n",
    "        obj = {\"Data\": idx, \"type\": key, \"KNN\": (acc_knn, f1_knn), \"RDF\": (acc_rdf, f1_rdf), \"MICE\": (acc_mice, f1_mice)}\n",
    "        list_40_res.append(obj)\n",
    "    #60% missing data\n",
    "    for key, value in list_60per_miss[idx].items():\n",
    "        if key == \"cat_cols\":\n",
    "            continue\n",
    "        knnimp_per60,  msForest_per60, mice_per60, label_set = fill_missing_split_norm_method(value, list_60per_miss[idx][\"cat_cols\"])\n",
    "        acc_knn, f1_knn = result_each_method(model_for_classification(len(set(label_set[0]))), knnimp_per60, label_set)\n",
    "        acc_rdf, f1_rdf = result_each_method(model_for_classification(len(set(label_set[0]))), msForest_per60, label_set)\n",
    "        acc_mice, f1_mice = result_each_method(model_for_classification(len(set(label_set[0]))), mice_per60, label_set)\n",
    "        obj = {\"Data\": idx, \"type\": key, \"KNN\": (acc_knn, f1_knn), \"RDF\": (acc_rdf, f1_rdf), \"MICE\": (acc_mice, f1_mice)}\n",
    "        list_60_res.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9aeed-5c5b-4f41-9f7e-ced175a56b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_20_res = json.load(open(\"Result/result_per20.json\"))\n",
    "list_40_res = json.load(open(\"Result/result_per40.json\"))\n",
    "list_60_res = json.load(open(\"Result/result_per60.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d820768-67d8-495f-baab-77972a15f10d",
   "metadata": {},
   "source": [
    "# Thử nghiệm với AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81008e2-092f-4c30-8fee-cf9a308c1303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, lr=0.01):\n",
    "    # sgd = Optimizer.SGD(learning_rate=lr, momentum=0.99, nesterov=True)\n",
    "    adam = Optimizer.Adam(learning_rate=lr)\n",
    "    model.compile(optimizer=adam, loss=tf.keras.losses.MeanSquaredError(), metrics=[tf.keras.metrics.mean_squared_error])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79c34df-5d90-46c2-b39d-e0a2e74c8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_ae(input_shape, type_model = \"dae\"):\n",
    "    if type_model == \"dae\":  \n",
    "        def buil_model_tuner_dae(hp):\n",
    "            model_dae= compile_model(DAE.create_model(input_size=input_shape, \n",
    "                                                      num_layer=hp.Choice('num_layer', [2, 3, 4], default=3, ordered=True),\n",
    "                                                      drop_out_rate=hp.Choice('dr_rate', [0.4, 0.5], default=0.5, ordered=True),\n",
    "                                                      theta=hp.Choice('theta', [5,7,9], default=7, ordered=True),\n",
    "                                                      activation_func=hp.Choice('acf', ['tanh', 'relu'], default='tanh', ordered=False)))\n",
    "            return model_dae\n",
    "        tuner = kt.BayesianOptimization(\n",
    "                    buil_model_tuner_dae,\n",
    "                    objective=kt.Objective(\"loss\", direction=\"min\"),\n",
    "                    max_trials=20\n",
    "                  )\n",
    "        return tuner\n",
    "    elif type_model == \"vae\" or type_model == \"cae\":\n",
    "         def buil_model_tuner_vae(hp):\n",
    "            units_layer_1 = hp.Choice('units_layer_1', [input_shape - 3, input_shape + 5, input_shape + 10], ordered=True) \n",
    "            units_layer_2 = hp.Choice('units_layer_2', [input_shape - 5, input_shape + 8, input_shape + 15], ordered=True) \n",
    "            latent_dim = hp.Choice('latent_dim', [5, 6 ,7], ordered=True)\n",
    "            model_vae= VAE.compile_model(VAE.create_model(input_size=input_shape, \n",
    "                                                          layer_units=[units_layer_1, units_layer_2],\n",
    "                                                          latent_dim=latent_dim,\n",
    "                                                          activation_func=hp.Choice('acf', ['tanh', 'relu'], default='relu', ordered=False)))\n",
    "            return model_vae\n",
    "        \n",
    "         tuner = kt.BayesianOptimization(\n",
    "                    buil_model_tuner_vae,\n",
    "                    objective=kt.Objective(\"total_loss\", direction=\"min\"),\n",
    "                    max_trials=20\n",
    "                      )\n",
    "         return tuner\n",
    "    elif type_model == \"cae\":\n",
    "        return None\n",
    "    else:\n",
    "        print(\"Không có mô hình AutoEncoder phù hợp\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f211ed9-be9c-47b2-a2b2-2dec342ee7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_split_ae_method(df, label='label'):\n",
    "    label_ = df[label]\n",
    "    df = df.drop(columns=[label])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, label_, test_size=0.2, random_state=209)\n",
    "    for each in X_train.columns.to_list():\n",
    "        X_train[each] = X_train[each].fillna(X_train[each].mean())\n",
    "        X_test[each] = X_test[each].fillna(X_train[each].mean())\n",
    "    tuner_dae = build_model_ae(X_train.shape[1], type_model='dae')\n",
    "    tuner_vae = build_model_ae(X_train.shape[1], type_model='vae')\n",
    "    tuner_dae.search(X_train, X_train, epochs=120, batch_size=32,\n",
    "                     callbacks=[EarlyStopping(monitor='loss', patience=20, mode='min'), \n",
    "                                ReduceLROnPlateau(monitor='loss', factor=0.8, patience=5, mode='min')], verbose=0)\n",
    "    tuner_vae.search(X_train.values, epochs=120, batch_size=32,\n",
    "                     callbacks=[EarlyStopping(monitor='total_loss', patience=20, mode='min'), \n",
    "                                ReduceLROnPlateau(monitor='total_loss', factor=0.8, patience=5, mode='min')], verbose=0)\n",
    "    best_model_dae = tuner_dae.get_best_models(1)[0]\n",
    "    best_model_vae = tuner_vae.get_best_models(1)[0]\n",
    "    train_fill_dae, val_fill_dae = best_model_dae.predict(X_train), best_model_dae.predict(X_test)\n",
    "    train_fill_vae, val_fill_vae = best_model_vae.predict(X_train), best_model_vae.predict(X_test)\n",
    "    return (train_fill_dae, val_fill_dae), (train_fill_vae, val_fill_vae), (y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01233a60-b24a-4b85-a806-66279d3a96be",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_20_res_ae, list_40_res_ae, list_60_res_ae = [], [], []\n",
    "save_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b73214-8419-41ff-a782-f129f58732f6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, each in enumerate(list_20per_miss):\n",
    "    for key, value in list_20per_miss[idx].items():\n",
    "        if key == \"cat_cols\":\n",
    "            continue\n",
    "        dae_fill, vae_fill,  label_set = fill_missing_split_ae_method(value)\n",
    "        acc_dae, f1_dae = result_each_method(model_for_classification(len(set(label_set[0]))), dae_fill, label_set)\n",
    "        acc_vae, f1_vae = result_each_method(model_for_classification(len(set(label_set[0]))), (vae_fill[0][0], vae_fill[1][0]), label_set)\n",
    "        obj = {\"Data\": idx, \"type\": key, \"DAE\": (acc_dae, f1_dae), \"VAE\": (acc_vae, f1_vae)}\n",
    "        save_data.append((dae_fill, vae_fill))\n",
    "        list_20_res_ae.append(obj)\n",
    "    #40% missng data\n",
    "    for key, value in list_40per_miss[idx].items():\n",
    "        if key == \"cat_cols\":\n",
    "            continue\n",
    "        dae_fill, vae_fill,  label_set = fill_missing_split_ae_method(value)\n",
    "        acc_dae, f1_dae = result_each_method(model_for_classification(len(set(label_set[0]))), dae_fill, label_set)\n",
    "        acc_vae, f1_vae = result_each_method(model_for_classification(len(set(label_set[0]))), (vae_fill[0][0], vae_fill[1][0]), label_set)\n",
    "        obj = {\"Data\": idx, \"type\": key, \"DAE\": (acc_dae, f1_dae), \"VAE\": (acc_vae, f1_vae)}\n",
    "        save_data.append((dae_fill, vae_fill))\n",
    "        list_40_res_ae.append(obj)\n",
    "    #60% missing data\n",
    "    for key, value in list_60per_miss[idx].items():\n",
    "        if key == \"cat_cols\":\n",
    "            continue\n",
    "        dae_fill, vae_fill,  label_set = fill_missing_split_ae_method(value)\n",
    "        acc_dae, f1_dae = result_each_method(model_for_classification(len(set(label_set[0]))), dae_fill, label_set)\n",
    "        acc_vae, f1_vae = result_each_method(model_for_classification(len(set(label_set[0]))), (vae_fill[0][0], vae_fill[1][0]), label_set)\n",
    "        obj = {\"Data\": idx, \"type\": key, \"DAE\": (acc_dae, f1_dae), \"VAE\": (acc_vae, f1_vae)}\n",
    "        save_data.append((dae_fill, vae_fill))\n",
    "        list_60_res_ae.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af23b3-c975-484d-a044-0982e9c4a6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Result/result_per60_ae.json.json\", \"w\") as outfile:\n",
    "    json.dump(list_60_res_ae, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e8ffa-a917-49b1-a19c-e723db7b3050",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = StandardScaler().fit_transform(df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dc07f6-46dc-454c-8580-744c96711ac8",
   "metadata": {},
   "source": [
    "# DuoLoss AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01105169-3f76-4d78-9373-efabb735524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_ae(num_class, input_shape):\n",
    "    def buil_model_tuner_duoloss(hp):\n",
    "        model_duoLoss = DuoLossAE.compile_model(DuoLossAE.create_model(num_class=num_class,\n",
    "                                                                      input_size=input_shape, \n",
    "                                                                      num_layer=hp.Choice('num_layer', [2, 3, 4], default=3, ordered=True),\n",
    "                                                                      drop_out_rate=hp.Choice('dr_rate', [0.4, 0.5], default=0.5, ordered=True),\n",
    "                                                                      theta=hp.Choice('theta', [5,7,9], default=7, ordered=True),\n",
    "                                                                      activation_func=hp.Choice('acf', ['tanh', 'relu'], default='tanh', ordered=False),\n",
    "                                                                      alpha=hp.Choice('alp', [0.4, 0.5, 0.6, 0.7], default=0.5, ordered=True)))\n",
    "        return model_duoLoss\n",
    "    tuner = kt.BayesianOptimization(\n",
    "                buil_model_tuner_duoloss,\n",
    "                objective=kt.Objective(\"total_loss\", direction=\"min\"),\n",
    "                max_trials=30\n",
    "              )\n",
    "    return tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4209137b-70e6-4efe-ad29-03ad91f689b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def duo_method(df, label='label'):\n",
    "    label_ = df[label]\n",
    "    df = df.drop(columns=[label])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, label_, test_size=0.2, random_state=209)\n",
    "    for each in X_train.columns.to_list():\n",
    "        X_train[each] = X_train[each].fillna(X_train[each].mean())\n",
    "        X_test[each] = X_test[each].fillna(X_train[each].mean())\n",
    "    tuner_duoLoss = build_model_ae(len(set(label_)), X_train.shape[1])\n",
    "    tuner_duoLoss.search(X_train.values, y_train, epochs=120, batch_size=32,\n",
    "                         callbacks=[EarlyStopping(monitor='total_loss', patience=20, mode='min'), \n",
    "                                    ReduceLROnPlateau(monitor='total_loss', factor=0.8, patience=5, mode='min')], verbose=0)\n",
    "    best_model_dae = tuner_duoLoss.get_best_models(1)[0]\n",
    "    pred = np.argmax(best_model_dae.predict(X_test)[1], axis=1)\n",
    "    return accuracy_score(y_test, pred), f1_score(y_test, pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595dff97-8a53-44d6-9ab9-c551f2ec0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_20_res_duo, list_40_res_duo, list_60_res_duo = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546dff50-2e2a-4b83-b923-87172b2e2925",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, each in enumerate(list_20per_miss):\n",
    "    for key, value in list_20per_miss[idx].items():\n",
    "        if key == \"cat_cols\":\n",
    "            continue\n",
    "        acc_duo, f1_duo = duo_method(value)\n",
    "        obj = {\"Data\": idx, \"type\": key, \"DAE_DUO\": (acc_duo, f1_duo)}\n",
    "        list_20_res_duo.append(obj)\n",
    "    #40% missng data\n",
    "    for key, value in list_40per_miss[idx].items():\n",
    "        if key == \"cat_cols\":\n",
    "            continue\n",
    "        acc_duo, f1_duo = duo_method(value)\n",
    "        obj = {\"Data\": idx, \"type\": key, \"DAE_DUO\": (acc_duo, f1_duo)}\n",
    "        list_40_res_duo.append(obj)\n",
    "    #60% missing data\n",
    "    for key, value in list_60per_miss[idx].items():\n",
    "        if key == \"cat_cols\":\n",
    "            continue\n",
    "        acc_duo, f1_duo = duo_method(value)\n",
    "        obj = {\"Data\": idx, \"type\": key, \"DAE_DUO\": (acc_duo, f1_duo)}\n",
    "        list_60_res_duo.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24bf18-2538-4241-b814-9e471506bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Result/result_per60_duo.json\", \"w\") as outfile:\n",
    "    json.dump(list_60_res_duo, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bcabfb-c438-4425-aabd-b2f489dfdba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_40_res_duo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803f0fc1-99c4-430f-a615-954884719f5b",
   "metadata": {},
   "source": [
    "# Tổng hợp kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0026ff-5513-4020-86f4-2e73e025bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_20, res_40, res_60 = json.load(open(\"Result/result_per20.json\")), json.load(open(\"Result/result_per40.json\")), json.load(open(\"Result/result_per60.json\"))\n",
    "res_20_ae, res_40_ae, res_60_ae = json.load(open(\"Result/result_per20_ae.json\")), json.load(open(\"Result/result_per40_ae.json\")), json.load(open(\"Result/result_per60_ae.json\"))\n",
    "res_20_duo, res_40_duo, res_60_duo = json.load(open(\"Result/result_per20_duo.json\")), json.load(open(\"Result/result_per40_duo.json\")), json.load(open(\"Result/result_per60_duo.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe26165d-304d-49fd-ad4b-be4e6552166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, each in enumerate(res_20):\n",
    "    for method in list(res_20_ae[idx].keys()):\n",
    "        if method not in [\"Data\", \"type\"]:\n",
    "            res_20[idx][method] = (res_20_ae[idx][method][0], res_20_ae[idx][method][1])\n",
    "    for method in list(res_40_ae[idx].keys()):\n",
    "        if method not in [\"Data\", \"type\"]:\n",
    "            res_40[idx][method] = (res_40_ae[idx][method][0], res_40_ae[idx][method][1])\n",
    "    for method in list(res_40_ae[idx].keys()):\n",
    "        if method not in [\"Data\", \"type\"]:\n",
    "            res_60[idx][method] = (res_60_ae[idx][method][0], res_60_ae[idx][method][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f198a-6f48-4678-a09b-0145a7bd801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(res_1, typ=\"mcar\"):\n",
    "    dict_data = {0: \"BC\", 1: \"CE\", 2: \"CMC\", 3: \"IM\", 4: \"TTT\"}\n",
    "    list_data, list_method, uniform_acc, uniform_f1, random_acc, random_f1 = [], [], [], [], [], []\n",
    "    for index, each in enumerate(res_1):\n",
    "        if typ == \"mcar\":\n",
    "            t1, t2 = 0, 1\n",
    "        else:\n",
    "            t1, t2 = 2, 3\n",
    "        if index % 4 == t1:\n",
    "            for method in list(each.keys()):\n",
    "                if method not in [\"Data\", \"type\"]:\n",
    "                    list_data.append(dict_data[each[\"Data\"]])\n",
    "                    list_method.append(method)\n",
    "                    uniform_acc.append(round(each[method][0], 3))\n",
    "                    uniform_f1.append(round(each[method][1], 3))\n",
    "        elif index % 4 == t2:\n",
    "            for method in list(each.keys()):\n",
    "                if method not in [\"Data\", \"type\"]:\n",
    "                    random_acc.append(round(each[method][0], 3))\n",
    "                    random_f1.append(round(each[method][1], 3))\n",
    "        else:\n",
    "            continue\n",
    "    return pd.DataFrame(\n",
    "        data = {\n",
    "            \"Dữ liệu\": list_data,\n",
    "            \"Phương pháp\": list_method,\n",
    "            \"Uniform_acc\": uniform_acc,\n",
    "            \"Uniform_f1\": uniform_f1,\n",
    "            \"Random_acc\": random_acc, \n",
    "            \"Random_f1\": random_f1\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72824d9b-be47-4395-afe1-c702f84592e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_df(res_20).to_csv(\"ResultCSV/20_per_mcar.csv\", index=False)\n",
    "create_df(res_20, typ=\"mar\").to_csv(\"ResultCSV/20_per_mar.csv\", index=False)\n",
    "create_df(res_40).to_csv(\"ResultCSV/40_per_mcar.csv\", index=False)\n",
    "create_df(res_40, typ=\"mar\").to_csv(\"ResultCSV/40_per_mar.csv\", index=False)\n",
    "create_df(res_60).to_csv(\"ResultCSV/60_per_mcar.csv\", index=False)\n",
    "create_df(res_60, typ=\"mar\").to_csv(\"ResultCSV/60_per_mar.csv\", index=False)\n",
    "# create_df(res_duo, typ=\"mar\").to_csv(\"ResultCSV/60_per_mar.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5604b90-ae13-492a-a4e9-34b4d6e037eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count_best(res, typ=0): \n",
    "    dict_max_count_mar = {\"KNN\": 0, \"MissForest\": 0, \"MICE\": 0, \"DAE\": 0, \"VAE\": 0}\n",
    "    dict_max_count_mcar = {\"KNN\": 0, \"MissForest\": 0, \"MICE\": 0, \"DAE\": 0, \"VAE\": 0}\n",
    "    for idx, each in enumerate(res):\n",
    "        max_value = 0\n",
    "        for method in list(each.keys()):\n",
    "            if method not in [\"Data\", \"type\"]:\n",
    "                if each[method][typ] > max_value: \n",
    "                    max_value = each[method][typ]\n",
    "        for method in list(each.keys()):\n",
    "            if method not in [\"Data\", \"type\"]:\n",
    "                if each[method][typ] == max_value:\n",
    "                    if \"mnar\" in each[\"type\"]:\n",
    "                        if method == \"RDF\":\n",
    "                            dict_max_count_mar[\"MissForest\"] += 1\n",
    "                            continue\n",
    "                        dict_max_count_mar[method] += 1\n",
    "                    else:\n",
    "                        if method == \"RDF\":\n",
    "                            dict_max_count_mcar[\"MissForest\"] += 1\n",
    "                            continue\n",
    "                        dict_max_count_mcar[method] += 1\n",
    "    return dict_max_count_mar, dict_max_count_mcar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ddad4-163b-450c-b5c2-0185f57d0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_max_count_20_mar, dict_max_count_20_mcar = get_count_best(res_20)\n",
    "dict_max_count_40_mar, dict_max_count_40_mcar = get_count_best(res_40)\n",
    "dict_max_count_60_mar, dict_max_count_60_mcar = get_count_best(res_60)\n",
    "\n",
    "dict_max_count_20_mar_f1, dict_max_count_20_mcar_f1 = get_count_best(res_20, 1)\n",
    "dict_max_count_40_mar_f1, dict_max_count_40_mcar_f1 = get_count_best(res_40, 1)\n",
    "dict_max_count_60_mar_f1, dict_max_count_60_mcar_f1 = get_count_best(res_60, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd1cfa7-2862-4272-8bec-25dadcea914d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_max_count_20_mcar = {\"AEE-DAE\": 6, \"KNN\": 2, \"MissForest\": 1, \"MICE\": 1, \"AEP-DAE\": 1}\n",
    "dict_max_count_20_mar = {\"AEE-DAE\": 4, \"KNN\": 1, \"MissForest\": 2, \"MICE\": 3, \"AEP-DAE\": 2}\n",
    "dict_max_count_20_mcar_f1 = {\"AEE-DAE\": 6, \"KNN\": 1, \"MissForest\": 1, \"MICE\": 1, \"AEP-DAE\": 1}\n",
    "dict_max_count_20_mar_f1 = {\"AEE-DAE\": 2, \"KNN\": 1, \"MissForest\": 3, \"MICE\": 3, \"AEP-DAE\": 2}\n",
    "\n",
    "dict_max_count_40_mcar = {\"AEE-DAE\": 6, \"KNN\": 2, \"MissForest\": 2, \"MICE\": 1, \"AEP-DAE\": 2}\n",
    "dict_max_count_40_mar = {\"AEE-DAE\": 4, \"KNN\": 0, \"MissForest\": 2, \"MICE\": 3, \"AEP-DAE\": 3}\n",
    "dict_max_count_40_mcar_f1 = {\"AEE-DAE\": 5, \"KNN\": 3, \"MissForest\": 2, \"MICE\": 1, \"AEP-DAE\": 0}\n",
    "dict_max_count_40_mar_f1 = {\"AEE-DAE\": 5, \"KNN\": 0, \"MissForest\": 1, \"MICE\": 4, \"AEP-DAE\": 1}\n",
    "\n",
    "dict_max_count_60_mcar = {\"AEE-DAE\": 4, \"KNN\": 2, \"MissForest\": 1, \"MICE\": 2, \"AEP-DAE\": 2}\n",
    "dict_max_count_60_mar = {\"AEE-DAE\": 4, \"KNN\": 3, \"MissForest\": 3, \"MICE\": 1, \"AEP-DAE\": 0}\n",
    "dict_max_count_60_mcar_f1 = {\"AEE-DAE\": 5, \"KNN\": 2, \"MissForest\": 1, \"MICE\": 1, \"AEP-DAE\": 2}\n",
    "dict_max_count_60_mar_f1 = {\"AEE-DAE\": 4, \"KNN\": 2, \"MissForest\": 3, \"MICE\": 1, \"AEP-DAE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9153c5-fb5f-427c-b5bb-1fadc0056a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacf886e-2a2a-4594-addd-5df904476b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(15, 20))\n",
    "X_axis = np.arange(len(list(dict_max_count_20_mcar.keys())))\n",
    "ax[0][0].bar(X_axis - 0.2, np.array(list(dict_max_count_20_mcar.values()), dtype=\"int\"), 0.4, label=\"Accuracy\", color=\"blue\")\n",
    "ax[0][0].bar(X_axis + 0.2, np.array(list(dict_max_count_20_mcar_f1.values()), dtype=\"int\"), 0.4, label=\"F1_score\", color=\"red\")\n",
    "ax[0][0].set_xticks(X_axis, list(dict_max_count_20_mcar.keys()))\n",
    "ax[0][0].legend()\n",
    "ax[0][0].set_title(\"MCAR\")\n",
    "X_axis = np.arange(len(list(dict_max_count_20_mar.keys())))\n",
    "ax[0][1].bar(X_axis - 0.2, np.array(list(dict_max_count_20_mar.values()), dtype=\"int\"), 0.4, label=\"Accuracy\", color=\"blue\")\n",
    "ax[0][1].bar(X_axis + 0.2, np.array(list(dict_max_count_20_mar_f1.values()), dtype=\"int\"), 0.4, label=\"F1_score\", color=\"red\")\n",
    "ax[0][1].set_xticks(X_axis, list(dict_max_count_20_mar.keys()))\n",
    "ax[0][1].legend()\n",
    "ax[0][1].set_title(\"MAR\")\n",
    "X_axis = np.arange(len(list(dict_max_count_40_mcar.keys())))\n",
    "ax[1][0].bar(X_axis - 0.2, np.array(list(dict_max_count_40_mcar.values()), dtype=\"int\"), 0.4, label=\"Accuracy\", color=\"blue\")\n",
    "ax[1][0].bar(X_axis + 0.2, np.array(list(dict_max_count_40_mcar_f1.values()), dtype=\"int\"), 0.4, label=\"F1_score\", color=\"red\")\n",
    "ax[1][0].set_xticks(X_axis, list(dict_max_count_40_mcar.keys()))\n",
    "ax[1][0].legend()\n",
    "ax[1][0].set_title(\"MCAR\")\n",
    "X_axis = np.arange(len(list(dict_max_count_40_mar.keys())))\n",
    "ax[1][1].bar(X_axis - 0.2, np.array(list(dict_max_count_40_mar.values()), dtype=\"int\"), 0.4, label=\"Accuracy\", color=\"blue\")\n",
    "ax[1][1].bar(X_axis + 0.2, np.array(list(dict_max_count_40_mar_f1.values()), dtype=\"int\"), 0.4, label=\"F1_score\", color=\"red\")\n",
    "ax[1][1].set_xticks(X_axis, list(dict_max_count_40_mar.keys()))\n",
    "ax[1][1].legend()\n",
    "ax[1][1].set_title(\"MAR\")\n",
    "X_axis = np.arange(len(list(dict_max_count_60_mcar.keys())))\n",
    "ax[2][0].bar(X_axis - 0.2, np.array(list(dict_max_count_60_mcar.values()), dtype=\"int\"), 0.4, label=\"Accuracy\", color=\"blue\")\n",
    "ax[2][0].bar(X_axis + 0.2, np.array(list(dict_max_count_60_mcar_f1.values()), dtype=\"int\"), 0.4, label=\"F1_score\", color=\"red\")\n",
    "ax[2][0].set_xticks(X_axis, list(dict_max_count_60_mcar.keys()))\n",
    "ax[2][0].legend()\n",
    "ax[2][0].set_title(\"MCAR\")\n",
    "X_axis = np.arange(len(list(dict_max_count_60_mar.keys())))\n",
    "ax[2][1].bar(X_axis - 0.2, np.array(list(dict_max_count_60_mar.values()), dtype=\"int\"), 0.4, label=\"Accuracy\", color=\"blue\")\n",
    "ax[2][1].bar(X_axis + 0.2, np.array(list(dict_max_count_60_mar_f1.values()), dtype=\"int\"), 0.4, label=\"F1_score\", color=\"red\")\n",
    "ax[2][1].set_xticks(X_axis, list(dict_max_count_60_mar.keys()))\n",
    "ax[2][1].legend()\n",
    "ax[2][1].set_title(\"MAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308eaa1-e4f7-4ab3-8615-868bbcdb2910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envtorch-gpu",
   "language": "python",
   "name": "envtorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
